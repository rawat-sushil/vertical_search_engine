{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "BUILD_INDEX = True\n",
    "BUILD_INDEX_TEST = False\n",
    "TEST_QUERY = False\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "virgin_index_creation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_INDEX: \n",
    "\n",
    "    def create_inverted_index(inputFile):\n",
    "#         stop =5 # for debugging\n",
    "        new_word = True\n",
    "        wordsAdded = {}\n",
    "\n",
    "        data_type = {'Document_Id' : 'int',\n",
    "                     'Profile_Name':'str',\n",
    "                     'Title':'str',\n",
    "                     'All_Authors':'str',\n",
    "                     'Title_Link': 'str',\n",
    "                     'No_Of_Citations':'str',\n",
    "                     'Year_Published':'str'\n",
    "                    }\n",
    "\n",
    "        data = pd.read_csv(inputFile,header='infer',dtype=data_type)\n",
    "\n",
    "        exlcude_puncuation = [';', '.', ',', '!', ':', '-','_']\n",
    "        #set stopwords for english language\n",
    "        sw = stopwords.words('english')\n",
    "\n",
    "        ps=PorterStemmer()\n",
    "        filtered_docs = []\n",
    "        docs = data['Profile_Name']+\" \"+data['Title']+\" \"+data['All_Authors']+\" \"+\" \"+data['Year_Published'].astype('str')# combine name title year for indexing\n",
    "#         print(docs)\n",
    "        for title,doc_id in zip(docs,data.Document_Id):\n",
    "            tokens = word_tokenize(str(title))\n",
    "            temp = ''\n",
    "            word_count =1\n",
    "#             print(\"\\n\",tokens,\"WC\",word_count)\n",
    "#             break\n",
    "            for word in tokens:\n",
    "                if word not in sw and word not in exlcude_puncuation and len(word)>2:\n",
    "                    word_stem =ps.stem(word) # stem the word\n",
    "                    if word_stem not in wordsAdded.keys(): # Create a new entry in dictionary for new words : terms and posting\n",
    "                        wordsAdded[word_stem]=[]\n",
    "                        word_count=1\n",
    "                        new_word =True\n",
    "                    else:\n",
    "                        new_word=False\n",
    "\n",
    "\n",
    "                    if new_word==True: # Adding word and count first time into dicttionary\n",
    "                        #print(\"inside ..\",word_stem,doc_id,word_count,\"\\n\")\n",
    "                        wordsAdded[word_stem].append((doc_id,word_count))\n",
    "                        word_count +=1 # increment the existing word count\n",
    "                    else: # only update the doc_id and count for word                        \n",
    "                        #find position of tuple to update count of word in the index dictionary\n",
    "                        val_length = len(wordsAdded[word_stem]) # find how many entries are already made for word\n",
    "#                         print(\"\\nAlready existing dict : \", word_stem, wordsAdded[word_stem])\n",
    "                        for count in range(val_length):\n",
    "                            if wordsAdded[word_stem][count][0] == doc_id : # find the location of tuple with matching doc id \n",
    "                                location=True\n",
    "                                position=count\n",
    "                                break # found the location of tuple for matching doc_id so break the loop\n",
    "\n",
    "                        if location == True:\n",
    "                            existing_word_count = wordsAdded[word_stem][position][1]\n",
    "                            wordsAdded[word_stem][position] = (doc_id, existing_word_count+1)\n",
    "                            #reset flags\n",
    "                            location = False\n",
    "                            position=-1\n",
    "                        else:  # its a new doc_id entry for existing word\n",
    "                            # add a new doc_id and count info\n",
    "                            wordsAdded[word_stem].append((doc_id, 1))\n",
    "\n",
    "#                         print(\"\\nAlready existing dict (After update): \", word_stem, wordsAdded[word_stem])\n",
    "                        #print(\"inside ..\",word_stem,doc_id+1,word_count,\"\\n\")\n",
    "                    temp = temp + word_stem + \" \"\n",
    "\n",
    "            filtered_docs.append(temp)\n",
    "#             stop=stop-1\n",
    "#             if stop <0: break\n",
    "        data['Filtered_title'] =  filtered_docs \n",
    "\n",
    "        return wordsAdded,data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update already created index\n",
    "# return the filtered new data and update the global inverted index\n",
    "#input dataframe with channged rows\n",
    "def update_inverted_index(data):\n",
    "    global inv_index\n",
    "    new_word = True\n",
    "    \n",
    "    exlcude_puncuation = [';', '.', ',', '!', ':', '-','_']\n",
    "    #set stopwords for english language\n",
    "    sw = stopwords.words('english')\n",
    "    ps=PorterStemmer()\n",
    "    filtered_docs = []    \n",
    "    \n",
    "    docs = data['Profile_Name']+\" \"+data['Title']+\" \"+data['All_Authors']+\" \"+\" \"+data['Year_Published'].astype('str')# combine name title year for indexing \n",
    "    \n",
    "    if virgin_index_creation == False:\n",
    "        if len(inv_index):\n",
    "            for title,doc_id in zip(docs,data.Document_Id):\n",
    "                tokens = word_tokenize(str(title))\n",
    "                temp = ''\n",
    "                word_count =1\n",
    "    #             print(\"\\n\",tokens,\"WC\",word_count)\n",
    "    #             break\n",
    "                for word in tokens:\n",
    "                    if word not in sw and word not in exlcude_puncuation and len(word)>2:\n",
    "                        word_stem =ps.stem(word) # stem the word\n",
    "                        if word_stem not in inv_index.keys(): # Create a new entry in dictionary for new words : terms and posting\n",
    "                            inv_index[word_stem]=[]\n",
    "                            word_count=1\n",
    "                            new_word =True\n",
    "                        else:\n",
    "                            new_word=False\n",
    "\n",
    "\n",
    "                        if new_word==True: # Adding word and count first time into dicttionary\n",
    "                            #print(\"inside ..\",word_stem,doc_id,word_count,\"\\n\")\n",
    "                            inv_index[word_stem].append((doc_id,word_count))\n",
    "                            word_count +=1 # increment the existing word count\n",
    "                        else: # only update the doc_id and count for word                        \n",
    "                            #find position of tuple to update count of word in the index dictionary\n",
    "                            val_length = len(inv_index[word_stem]) # find how many entries are already made for word\n",
    "    #                         print(\"\\nAlready existing dict : \", word_stem, wordsAdded[word_stem])\n",
    "                            for count in range(val_length):\n",
    "                                if inv_index[word_stem][count][0] == doc_id : # find the location of tuple with matching doc id \n",
    "                                    location=True\n",
    "                                    position=count\n",
    "                                    break # found the location of tuple for matching doc_id so break the loop\n",
    "\n",
    "                            if location == True:\n",
    "                                existing_word_count = inv_index[word_stem][position][1]\n",
    "                                inv_index[word_stem][position] = (doc_id, existing_word_count+1)\n",
    "                                #reset flags\n",
    "                                location = False\n",
    "                                position=-1\n",
    "                            else:  # its a new doc_id entry for existing word\n",
    "                                # add a new doc_id and count info\n",
    "                                inv_index[word_stem].append((doc_id, 1))\n",
    "\n",
    "    #                         print(\"\\nAlready existing dict (After update): \", word_stem, wordsAdded[word_stem])\n",
    "                            #print(\"inside ..\",word_stem,doc_id+1,word_count,\"\\n\")\n",
    "                        temp = temp + word_stem + \" \"\n",
    "\n",
    "                filtered_docs.append(temp)\n",
    "    #             stop=stop-1\n",
    "    #             if stop <0: break\n",
    "                data['Filtered_title'] =  filtered_docs \n",
    "            print(\"Update Completed Successfully...\")\n",
    "            return data           \n",
    "         \n",
    "        else:\n",
    "            print(\"Inverted index is empty..please first create the index\")            \n",
    "    else:\n",
    "        print(\"Please first create inverted index..\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Index\n",
    "### 1. Read the scrapped data from csv and create inverted index for the first time else only update the index for changed entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken to build index : 52.9759 sec\n",
      "Index Size 51111\n"
     ]
    }
   ],
   "source": [
    "if BUILD_INDEX:\n",
    "    if virgin_index_creation:  # create whole index for the first time\n",
    "        articles_inputFile = \"CoventryUni_Data_Scraped_articles.csv\"\n",
    "        start_time = time.time() # measure time taken to build index \n",
    "\n",
    "        display_data = pd.read_csv(articles_inputFile,header='infer')\n",
    "        display_data.fillna(value=0,inplace=True)\n",
    "        inv_index, data =create_inverted_index(articles_inputFile)\n",
    "\n",
    "        stop_time = time.time() # stop the timer\n",
    "        time_taken = stop_time-start_time # time taken to build index\n",
    "        print(\"\\nTime taken to build index :\",str(np.round(time_taken,4))+\" sec\") \n",
    "        print(\"Index Size\", len(inv_index))\n",
    "        virgin_index_creation = False\n",
    "        boot_file = open(\"boot.txt\",'w')\n",
    "        boot_file.write(\"0\")\n",
    "        boot_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index update main function\n",
    "def index_update():\n",
    "    read_file = open(\"boot.txt\",'r')\n",
    "    flag = read_file.read()\n",
    "    if int(flag) ==0:\n",
    "        print(\"Index update only\")\n",
    "\n",
    "        # Compare the data of last crawl with latest crawl data\n",
    "        last_file = \"CoventryUni_Data_Scraped_articles_last.csv\"\n",
    "        latest_file = \"CoventryUni_Data_Scraped_articles.csv\"\n",
    "\n",
    "        last_data = pd.read_csv(last_file,header='infer')\n",
    "        latest_data = pd.read_csv(latest_file, header='infer')\n",
    "\n",
    "        #check number of rows in each of the data\n",
    "        size_last = len(last_data)\n",
    "        size_latest= len(latest_data)\n",
    "\n",
    "        print(\"Old File size : \",size_last)\n",
    "        print(\"New File size : \",size_latest)\n",
    "        \n",
    "        if size_last==size_latest:\n",
    "            difference = latest_data[last_data.ne(latest_data).any(axis=1)]\n",
    "\n",
    "        elif size_last > size_latest:\n",
    "            difference = last_data[latest_data.ne(last_data).any(axis=1)]\n",
    "        else:\n",
    "            difference = latest_data[last_data.ne(latest_data).any(axis=1)]\n",
    "        \n",
    "        filtered_data = update_inverted_index(difference)\n",
    "        return filtered_data\n",
    "    else:\n",
    "        print(\"No update performed as index flag is not set to 0\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test index\n",
    "if BUILD_INDEX_TEST:\n",
    "\n",
    "    term ='machine'\n",
    "    start_time = time.time() # measure time taken to search data     \n",
    "    ps=PorterStemmer()\n",
    "\n",
    "    test_word =ps.stem(term)\n",
    "    \n",
    "    if test_word in inv_index.keys():\n",
    "        posting =inv_index[test_word]\n",
    "\n",
    "        stop_time = time.time() # stop the timer\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to search term in Index :\",str(np.round(time_taken,4))+\" sec\") \n",
    "        #test index\n",
    "    #     print(posting)\n",
    "        if len(posting)!= 0:\n",
    "            print(\"Searched term: \",term)\n",
    "            print(\"Total matching documents (\",len(posting),\") are found::\")\n",
    "            for position,_ in zip(range(len(posting)),range(3)): \n",
    "                doc_id = posting[position][0]\n",
    "                print(\"\\nDoc_Id: \",doc_id,end=' : ')\n",
    "                print(\"\\nDoc_Link: \",list(display_data[display_data['Document_Id'] == doc_id]['Title_Link']))\n",
    "                print(\"Title : \", list(display_data[display_data['Document_Id'] == doc_id]['Title']))\n",
    "                print(\"Profile Name : \", list(display_data[display_data['Document_Id'] == doc_id]['Profile_Name']))\n",
    "                print(\"Authors Name : \", list(display_data[display_data['Document_Id'] == doc_id]['All_Authors']))\n",
    "                print(\"No.Of Citations : \", list(display_data[display_data['Document_Id'] == doc_id]['No_Of_Citations']))\n",
    "                print(\"Publication Year : \", int(display_data[display_data['Document_Id'] == doc_id]['Year_Published']))   \n",
    "                \n",
    "    else:\n",
    "        print(\"Searched Term: \",test_word)\n",
    "        print(\"No Match found in the index..\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Processor engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate ranking\n",
    "def calc_doc_score(term):\n",
    "    document_id = []\n",
    "    document_score = []\n",
    "    df=pd.DataFrame(columns=['Document_Id','Document_Score'],index=None)\n",
    "    if term in inv_index.keys():\n",
    "        posting  = inv_index[term]\n",
    "        for doc_id in posting:\n",
    "            tf = calc_tf(term,doc_id[0])\n",
    "            idf = calc_idf(term)\n",
    "            score_idf = calc_tf_idf(tf,idf)\n",
    "            document_id.append(doc_id[0])\n",
    "            document_score.append(score_idf)             \n",
    "\n",
    "        df['Document_Id'],df['Document_Score']=document_id,document_score\n",
    "        return df\n",
    "    else:\n",
    "        return df # empty data frame\n",
    "    \n",
    "    \n",
    "#calculate ter frequency \n",
    "#TF(t,d) =  Log10 (1 + frequency of t in d)\n",
    "def calc_tf(term,document):\n",
    "    global inv_index\n",
    "    if term in inv_index.keys():\n",
    "        for i in range(len(inv_index[term])):\n",
    "            if inv_index[term][i][0] == document:\n",
    "#                 print(term, inv_index[term][i][1])\n",
    "                return ( round(math.log10(1+inv_index[term][i][1]),4))\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "#calculate inverted document frequency ( idf)\n",
    "#IDF(t) = log10 (N/(df(t))\n",
    "def calc_idf(term):\n",
    "    global inv_index\n",
    "    if term in inv_index.keys():\n",
    "        total_no_of_documents = max(inv_index.values())[-1][0] # find total no.of docuements\n",
    "\n",
    "        document_freq = count_word_frequency(inv_index[term])[0] # find document containing terms\n",
    "#         print(total_no_of_documents, document_freq)\n",
    "        return  math.log(total_no_of_documents/document_freq)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#calcuate score tf.tdf \n",
    "def calc_tf_idf(tf, idf):\n",
    "#     print(tf, idf,tf*idf)\n",
    "    return round(tf*idf,4)\n",
    "\n",
    "#returns word frequency and and no. of documents \n",
    "def count_word_frequency(posting):\n",
    "    word_freq =0\n",
    "    for inv_index in range(len(posting)):\n",
    "        word_freq = word_freq+posting[inv_index][1]\n",
    "        \n",
    "    return(len(posting),word_freq)\n",
    "\n",
    "\n",
    "#update document score for all the terms in a query\n",
    "def update_document_score_for_query(term_doc_score):\n",
    "    global document_score_query \n",
    "    \n",
    "    for doc_id,doc_score in zip(term_doc_score['Document_Id'],term_doc_score['Document_Score']) :\n",
    "#         print(\"Doc_id: \",doc_id,doc_score)  \n",
    "        if doc_id not in document_score_query.keys():\n",
    "            document_score_query[doc_id] = doc_score\n",
    "        else:\n",
    "#             print(\"Old score: \",  document_score_query[doc_id])\n",
    "            document_score_query[doc_id] = document_score_query[doc_id]+doc_score\n",
    "#             print(\"New score: \",  document_score_query[doc_id])\n",
    " \n",
    "# global query score dictionary\n",
    "document_score_query = {\"doc_id\":[],\"doc_score\":[]}\n",
    "\n",
    "def process_query(query):\n",
    "    # initialize STEM and stopwords\n",
    "    global inv_index\n",
    "    not_found = []\n",
    "    processed = False\n",
    "    ps=PorterStemmer()\n",
    "    exlcude_puncuation = [';', '.', ',', '!', ':', '-','_']\n",
    "    #set stopwords for english language\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # tokenize\n",
    "    token = word_tokenize(query)\n",
    "    term_freq_table = {}\n",
    "    # remove sropwords\n",
    "    for word in token:\n",
    "        if word not in stop_words and word not in exlcude_puncuation:        \n",
    "        # Stem the word\n",
    "            stem_word = ps.stem(word)\n",
    "#             print(stem_word)\n",
    "#             print(\"Term: \", stem_word)\n",
    "            print(stem_word, end=' ')\n",
    "            if stem_word in inv_index.keys():\n",
    "                processed = True\n",
    "                posting = inv_index[stem_word]\n",
    "                #print(\"Index: \", posting)\n",
    "                if stem_word not in term_freq_table.keys():\n",
    "                    term_freq_table[stem_word] = count_word_frequency(posting)[1]\n",
    "                    term_doc_score = calc_doc_score(stem_word)\n",
    "#                     print(stem_word, end='+')\n",
    "                    update_document_score_for_query(term_doc_score)\n",
    "\n",
    "    #             print(stem_word,\" Occurred in \", count_word_frequency(posting)[0], \\\n",
    "    #                   \" documents\", count_word_frequency(posting)[1] ,\" times\\n\")\n",
    "            else:\n",
    "                not_found.append(stem_word)\n",
    "    if len(not_found)!=0:\n",
    "        print(\"\\nNo match found for : \", not_found)\n",
    "    return processed,not_found\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Query processor engine\n",
    "if TEST_QUERY:\n",
    "\n",
    "    query = \"mason sushil\"\n",
    "    document_score_query.clear()\n",
    "\n",
    "    start_time = time.time() # measure time taken to search data   \n",
    "    print(\"Input Query ->\",query)\n",
    "    print(\"Processing Query :\", end=':')\n",
    "    result,not_found = process_query(query)\n",
    "    stop_time = time.time() # stop the timer\n",
    "    time_taken = stop_time-start_time\n",
    "    print(\"\\n\\nFound \",len(document_score_query), \"results\", \"in\",str(np.round(time_taken,4))+\" sec\" )\n",
    "    results = {}\n",
    "    if result == True:\n",
    "    #     print(document_score_query)\n",
    "        for i, key in zip(range(len(document_score_query)),sorted(document_score_query, key=document_score_query.get,reverse=True)):\n",
    "            print(\"\\nDoc_Id: \",key, \"Relevance Score: \",round(document_score_query[key],4))\n",
    "            print(\"Title: \",display_data.iloc[key-1]['Title'])\n",
    "            print(\"Title Link: \", display_data.iloc[key-1]['Title_Link'])\n",
    "            print(\"Profile Name: \",display_data.iloc[key-1]['Profile_Name'] )        \n",
    "            print(\"All Authors: \",display_data.iloc[key-1]['All_Authors'])\n",
    "            print(\"Year Published: \",int(display_data.iloc[key-1]['Year_Published']))   \n",
    "            results[key] = [round(document_score_query[key],4),\\\n",
    "                            display_data.iloc[key-1]['Title'],\\\n",
    "                            display_data.iloc[key-1]['Title_Link'],\\\n",
    "                            display_data.iloc[key-1]['Profile_Name'],\\\n",
    "                            display_data.iloc[key-1]['All_Authors'],\\\n",
    "                            int(display_data.iloc[key-1]['Year_Published']),\\\n",
    "                            display_data.iloc[key-1]['No_Of_Citations']]       \n",
    "            break\n",
    "    else:\n",
    "        print(\"No matching data found...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template,request\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subject classification  pre-requisite\n",
    "def subject_classify_pre(query):\n",
    "    #load the subject classification model\n",
    "    model_name = 'subject_Classification_NB.sav'\n",
    "    subject_model = pickle.load(open(model_name, 'rb')) \n",
    "    #Load training data to create vectorspace\n",
    "    x_train = pickle.load(open(\"Training_data.npy\",'rb'))\n",
    "#     print(x_train.shape,type(x_train))\n",
    "    # Form tf-idf vector\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    x_train_vector = vectorizer.fit_transform(x_train)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #convert into tokens, remove stop words and stem the tokens\n",
    "    tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n",
    "    test_input = tokenizer.tokenize(str(query))\n",
    "    \n",
    "    test_input = [ps.stem(token) for token in test_input if token not in stop_words]      \n",
    "    test_input =  [' '.join(map(str,test_input))]\n",
    "    \n",
    "    print(\"Inside function:\", test_input)\n",
    "    test_input = np.array(test_input)\n",
    "    test_vector = vectorizer.transform(test_input)        \n",
    "\n",
    "    return subject_model, test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [30/Mar/2021 11:18:32] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search items\n",
      "geographies of asian\n",
      "Input Query -> geographies of asian\n",
      "Processing Query ::geographi asian "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Mar/2021 11:18:40] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Found  105 results in 1.495 sec\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def searchEngine():\n",
    "    query = []\n",
    "    errors = []\n",
    "    results={}\n",
    "    virgin_boot = False\n",
    "    stats = []\n",
    "    not_found = []\n",
    "    search = True\n",
    "    subject=[]\n",
    "    subject_mapping = {0:\"Business\",\n",
    "                       1:\"Entertainment\",\n",
    "                       2:\"Politics\",\n",
    "                       3:\"Sport\",\n",
    "                       4:\"Tech\"} \n",
    "    \n",
    "\n",
    "    #if user has entered the search query\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            #get user query\n",
    "            query = request.form['query_form']\n",
    "            \n",
    "            if request.form['search']=='search':\n",
    "                print(\"Search items\")\n",
    "                search =True\n",
    "            elif request.form['search']=='classify':\n",
    "                print(\"Classification of subject\")\n",
    "                search=False\n",
    "            \n",
    "            document_score_query.clear()\n",
    "            results.clear()              \n",
    "            stats.clear()\n",
    "            not_found.clear()\n",
    "            subject=[]    \n",
    "            print(query)\n",
    "            \n",
    "        except:\n",
    "            errors.append(\"Unable to process query. Please submit query again.\")\n",
    "            return render_template('index.html',errors = errors)\n",
    "        \n",
    "        if query and search==True:\n",
    "            \n",
    "            virgin_boot = True\n",
    "            start_time = time.time() # measure time taken to search data \n",
    "            print(\"Input Query ->\",query)\n",
    "            print(\"Processing Query :\", end=':')\n",
    "            search,not_found = process_query(str(query))\n",
    "            stop_time = time.time() # stop the timer\n",
    "            time_taken = stop_time-start_time\n",
    "            print(\"\\n\\nFound \",len(document_score_query), \"results\", \"in\",str(np.round(time_taken,4))+\" sec\" )\n",
    "            stats.append(len(document_score_query)) # no. of documents found\n",
    "            stats.append(np.round(time_taken,4)) # time taken to process query\n",
    "#             print(stats)\n",
    "            if search == True:\n",
    "            #     print(document_score_query)\n",
    "                for i, key in zip(range(len(document_score_query)),sorted(document_score_query, key=document_score_query.get,reverse=True)):\n",
    "#                     print(\"\\nDoc_Id: \",key, \"Relevance Score: \",round(document_score_query[key],4))\n",
    "#                     print(\"Title: \",display_data.iloc[key-1]['Title'])\n",
    "#                     print(\"Title Link: \", display_data.iloc[key-1]['Title_Link'])\n",
    "#                     print(\"Profile Name: \",display_data.iloc[key-1]['Profile_Name'] )        \n",
    "#                     print(\"All Authors: \",display_data.iloc[key-1]['All_Authors'])\n",
    "#                     print(\"Year Published: \",int(display_data.iloc[key-1]['Year_Published']))\n",
    "                    results[key] = [round(document_score_query[key],4),\\\n",
    "                                    display_data.iloc[key-1]['Title'],\\\n",
    "                                    display_data.iloc[key-1]['Title_Link'],\\\n",
    "                                    display_data.iloc[key-1]['Profile_Name'],\\\n",
    "                                    display_data.iloc[key-1]['All_Authors'],\\\n",
    "                                    int(display_data.iloc[key-1]['Year_Published']),\\\n",
    "                                    display_data.iloc[key-1]['No_Of_Citations'] ]\n",
    "            else:\n",
    "                results = ''\n",
    "        else:\n",
    "            results = ''  # set null for search result\n",
    "            print(\"Query : \",query)\n",
    "            subject_model,test_vector = subject_classify_pre(query)\n",
    "            \n",
    "            result = subject_model.predict(test_vector)\n",
    "            prob = subject_model.predict_proba(test_vector)\n",
    "            print(prob*100)\n",
    "            subject.append(subject_mapping[result[0]])\n",
    "            subject.append(round(prob[0][np.argmax(prob)]*100,2))\n",
    "    return render_template('index.html',errors = errors, results=results,\\\n",
    "                           virgin_boot=virgin_boot,stats=stats,not_found=not_found,\\\n",
    "                          query = query,subject=subject)\n",
    "\n",
    "print(__name__)\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document_Id        2418\n",
       "Profile_Name       2418\n",
       "Title              2418\n",
       "All_Authors        2418\n",
       "Title_Link         2418\n",
       "No_Of_Citations    2418\n",
       "Year_Published     2418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_data[display_data['Year_Published']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
