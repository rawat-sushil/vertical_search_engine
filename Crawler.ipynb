{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CRAWL_PROFILE = True\n",
    "CRAWL_ARTICLE = True\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 30 23:18:55 2021\n"
     ]
    }
   ],
   "source": [
    "seed_url='https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779'\n",
    "base_url='https://scholar.google.co.uk'\n",
    "articles_inputFile = \"C:/Users/ruman/Downloads/Sci-Kit_Learning/7071\\CoventryUni_Data_Scraped_articles.csv\"\n",
    "\n",
    "queue= [seed_url]\n",
    "already_visited=[]\n",
    "total_entries= 0\n",
    "last_page_entries =0\n",
    "profile_id = 1\n",
    "document_id =1\n",
    "data_table = {}\n",
    "article_table = {}\n",
    "\n",
    "crawl_date = time.ctime()\n",
    "print(crawl_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save crawled data into csv file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Profile Data into CSV file\n",
    "def writeData_profile():\n",
    "    global data_table\n",
    "    col_names=['Profile_Name','Profile_Link','Designation','Interested_In','No_Of_Citations']\n",
    "\n",
    "    data_frame=pd.DataFrame.from_dict(data_table,orient ='index',columns=col_names)\n",
    "    data_frame.index.rename('Profile_Id', inplace=True)\n",
    "    \n",
    "    file_exists = os.path.isfile(\"CoventryUni_Data_Scraped_profile.csv\")\n",
    "    print(file_exists)\n",
    "    if file_exists: #backup the file\n",
    "        shutil.copy(\"CoventryUni_Data_Scraped_profile.csv\", \"CoventryUni_Data_Scraped_profile_last.csv\")  \n",
    "        \n",
    "    data_frame.to_csv(\"CoventryUni_Data_Scraped_profile.csv\")\n",
    "\n",
    "\n",
    "#Write Article Data into CSV file\n",
    "def writeData_articles():\n",
    "    global article_table\n",
    "    col_names=['Profile_Name','Title','All_Authors','Title_Link','No_Of_Citations','Year_Published']\n",
    "\n",
    "    data_frame=pd.DataFrame.from_dict(article_table,orient='index',columns=col_names)\n",
    "    data_frame.index.rename('Document_Id', inplace=True)\n",
    "    \n",
    "    file_exists = os.path.isfile(\"CoventryUni_Data_Scraped_articles.csv\")\n",
    "    print(file_exists)\n",
    "    if file_exists: #backup the file\n",
    "        shutil.copy(\"CoventryUni_Data_Scraped_articles.csv\", \"CoventryUni_Data_Scraped_articles_last.csv\") \n",
    "        \n",
    "    data_frame.to_csv(\"CoventryUni_Data_Scraped_articles.csv\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Next page link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input : data structure representing parsed HTML text ( obtained from BeautifulSoup API)\n",
    "# returns link of next page\n",
    "def extract_next_page_link(soup):\n",
    "    global seed_url\n",
    "    global total_entries\n",
    "    #find the html tags for next page from html page\n",
    "    next_page=soup.find('button',attrs={'aria-label':'Next'})['onclick']\n",
    "\n",
    "    # extract the keys for next page - \n",
    "    next_page_keys= str.split(next_page,sep='\\\\')[-4:]\n",
    "    #Append to seed url to navigate to next \n",
    "    next_page_link = seed_url+'&'+next_page_keys[0][3:]+\"=\"+next_page_keys[1][3:]+ \\\n",
    "                               \"&\"+next_page_keys[2][3:]+\"=\"+str(total_entries)\n",
    "\n",
    "#     print(next_page_keys)\n",
    "#     print(next_page_link)\n",
    "    return next_page_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all the Names and required information in a page - Main landling page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input : page-class:`Response <Response>` object from GET request. \n",
    "# Update profile information in the global data_table\n",
    "def extract_info_from_page(page):\n",
    "    global base_url\n",
    "    global total_entries\n",
    "    global queue\n",
    "    global last_page_entries\n",
    "    global data_table\n",
    "    global profile_id\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    users= soup.find_all('div',attrs={'class':'gsc_1usr'})\n",
    "    no_of_entries_in_page = 0\n",
    "    for user in users:\n",
    "        profile_name=user.find('h3',attrs={'class':'gs_ai_name'})\n",
    "        link = user.find('a')['href']\n",
    "        designation =user.find('div',attrs={'class':'gs_ai_aff'})\n",
    "        interests = user.find('div',attrs={'class':'gs_ai_int'})\n",
    "        no_of_citation = user.find('div',{'class':'gs_ai_cby'})\n",
    "        no_of_entries_in_page +=1\n",
    "\n",
    "        if no_of_citation.text != '':\n",
    "            citation = str.split(no_of_citation.text,sep=' ')[2]\n",
    "        else:\n",
    "            citation =0\n",
    "        \n",
    "        data_table[profile_id] = [profile_name.text,base_url+link,designation.text,interests.text,citation]\n",
    "        \n",
    "        profile_id =profile_id+1\n",
    "        \n",
    "#         print(\"User Name: \",name.text)\n",
    "#         print(\"Link : \",base_url+link)\n",
    "#         print(\"Designation :\",designation.text)\n",
    "#         print(\"Interests : \",interests.text)\n",
    "#         print(\"No.of Citations :\",str.split(no_of_citation.text,sep=' ')[2])\n",
    "#         print(\"\\n\")\n",
    "\n",
    "    if last_page_entries == 0:\n",
    "        last_page_entries = no_of_entries_in_page\n",
    "\n",
    "    if no_of_entries_in_page <last_page_entries : # We have reached to last page\n",
    "        total_entries=total_entries+no_of_entries_in_page\n",
    "    else:    \n",
    "        total_entries=total_entries+no_of_entries_in_page\n",
    "        next_page= extract_next_page_link(soup) # still more pages left\n",
    "        queue.append(next_page)\n",
    "        last_page_entries = no_of_entries_in_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract individual profile details like Papers published, year etc..- Author Landing Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input - Profile link\n",
    "# output - update global article_table with all the details related with the profile \n",
    "def extract_user_info(profile_url):\n",
    "    global article_table\n",
    "    global document_id\n",
    "    start = 0\n",
    "    page_size=20    \n",
    "    #access url of profile \n",
    "    page =requests.get(profile_url)   \n",
    "    \n",
    "    if page.status_code != 200:\n",
    "        print(\"Failed to access url..[ERROR_CODE]:\", page.status_code)\n",
    "        print(\"Page Url : \",profile_url)\n",
    "        raise Exception(\"Error loading page..\")\n",
    "\n",
    "    else:\n",
    "        #read the page HTML content\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        #find  all the articles on single page\n",
    "        name = soup.find('div',attrs={'id':'gsc_prf_in'})\n",
    "        titles = soup.find_all('td',attrs={'class':'gsc_a_t'})\n",
    "        \n",
    "#         print(len(name),len(titles))\n",
    "        while len(titles) > 0: # NOT NULL\n",
    "        \n",
    "            no_of_citations = soup.find_all('td',attrs={'class':'gsc_a_c'})\n",
    "            year_Published = soup.find_all('td',attrs={'class':'gsc_a_y'})          \n",
    "            \n",
    "            for title, citation,year in zip(titles,no_of_citations,year_Published):\n",
    "#                 temp_title=title.find('a').text+\" \"+title.find('div',attrs={'class':'gs_gray'}).text #title +Authors\n",
    "                \n",
    "                profile_name = name.text\n",
    "#                 print(\"Profile: \",profile_name)\n",
    "                title_name = title.find('a').text\n",
    "#                 print(\"Title: \",title_name)\n",
    "                all_authors = title.find('div',attrs={'class':'gs_gray'}).text\n",
    "#                 print(\"All authors: \",all_authors)                \n",
    "                title_link = base_url+title.find('a')['data-href']\n",
    "#                 print(\"link: \",title_link)                \n",
    "                no_Of_Citations = citation.text\n",
    "#                 print(\"citation: \",no_Of_Citations)                \n",
    "                year_published = year.text\n",
    "#                 print(\"Year published : \", year_published)\n",
    "                \n",
    "                article_table[document_id] = [profile_name,title_name,all_authors,title_link,no_Of_Citations,year_published]\n",
    "\n",
    "                document_id=document_id+1 # increment the document id for next item\n",
    "            \n",
    "            start = start+page_size # show more pages of articles\n",
    "            if start != 100:\n",
    "                page_size = 80\n",
    "            else:\n",
    "                page_size = 100\n",
    "\n",
    "            new_page = requests.get(profile_url+'&cstart='+str(start)+'&pagesize='+str(page_size))\n",
    "            soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "            titles = soup.find_all('td',attrs={'class':'gsc_a_t'})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "page = requests.get(seed_url)\n",
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawl and extract all the profile related with coventry univertiy authors on google scholar\n",
    "## 2. Crawl and extract all the papers published by those authors from coventry university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.Crawling all the pages and extracting profile links ...\n",
      "\n",
      "Seed URL :  https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779\n",
      "......................................................................\n",
      "Time taken to scrape Profile data : 77.8738 sec\n",
      "\n",
      "Saving all the extracted profiles and articles into csv file..\n",
      "True\n",
      "\n",
      "Saving Completed.\n",
      "\n",
      "n 2. Crawling all the Profile links and extracting articles info ...\n",
      "\n",
      "First profile link :  https://scholar.google.co.uk/citations?hl=en&user=ywiPqccAAAAJ\n",
      "\n",
      "Queue length :  698\n",
      "697,696,695,694,693,692,691,690,689,688,687,686,685,684,683,682,681,680,679,678,677,676,675,674,673,672,671,670,669,668,667,666,665,664,663,662,661,660,659,658,657,656,655,654,653,652,651,650,649,648,647,646,645,644,643,642,641,640,639,638,637,636,635,634,633,632,631,630,629,628,627,626,625,624,623,622,621,620,619,618,617,616,615,614,613,612,611,610,609,608,607,606,605,604,603,602,601,600,599,598,597,596,595,594,593,592,591,590,589,588,587,586,585,584,583,582,581,580,579,578,577,576,575,574,573,572,571,570,569,568,567,566,565,564,563,562,561,560,559,558,557,556,555,554,553,552,551,550,549,548,547,546,545,544,543,542,541,540,539,538,537,536,535,534,533,532,531,530,529,528,527,526,525,524,523,522,521,520,519,518,517,516,515,514,513,512,511,510,509,508,507,506,505,504,503,502,501,500,499,498,497,496,495,494,493,492,491,490,489,488,487,486,485,484,483,482,481,480,479,478,477,476,475,474,473,472,471,470,469,468,467,466,465,464,463,462,461,460,459,458,457,456,455,454,453,452,451,450,449,448,447,446,445,444,443,442,441,440,439,438,437,436,435,434,433,432,431,430,429,428,427,426,425,424,423,422,421,420,419,418,417,416,415,414,413,412,411,410,409,408,407,406,405,404,403,402,401,400,399,398,397,396,395,394,393,392,391,390,389,388,387,386,385,384,383,382,381,380,379,378,377,376,375,374,373,372,371,370,369,368,367,366,365,364,363,362,361,360,359,358,357,356,355,354,353,352,351,350,349,348,347,346,345,344,343,342,341,340,339,338,337,336,335,334,333,332,331,330,329,328,327,326,325,324,323,322,321,320,319,318,317,316,315,314,313,312,311,310,309,308,307,306,305,304,303,302,301,300,299,298,297,296,295,294,293,292,291,290,289,288,287,286,285,284,283,282,281,280,279,278,277,276,275,274,273,272,271,270,269,268,267,266,265,264,263,262,261,260,259,258,257,256,255,254,253,252,251,250,249,248,247,246,245,244,243,242,241,240,239,238,237,236,235,234,233,232,231,230,229,228,227,226,225,224,223,222,221,220,219,218,217,216,215,214,213,212,211,210,209,208,207,206,205,204,203,202,201,200,199,198,197,196,195,194,193,192,191,190,189,188,187,186,185,184,183,182,181,180,179,178,177,176,175,174,173,172,171,170,169,168,167,166,165,164,163,162,161,160,159,158,157,156,155,154,153,152,151,150,149,148,147,146,145,144,143,142,141,140,139,138,137,136,135,134,133,132,131,130,129,128,127,126,125,124,123,122,121,120,119,118,117,116,115,114,113,112,111,110,109,108,107,106,105,104,103,102,101,100,99,98,97,96,95,94,93,92,91,90,89,88,87,86,85,84,83,82,81,80,79,78,77,76,75,74,73,72,71,70,69,68,67,66,65,64,63,62,61,60,59,58,57,56,55,54,53,52,51,50,49,48,47,46,45,44,43,42,41,40,39,38,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0,\n",
      "Time taken to scrape article data : 1618.2351 sec\n",
      "Nothing to process. Queue Len :  0\n",
      "No.Of Entries extracted :  698\n",
      "\n",
      "Completed before hard stop, Iterations still left before hard stop(max 500 iterations) :  0\n",
      "\n",
      "Saving all the extracted articles details into csv file..\n",
      "True\n",
      "\n",
      "Saving Completed.\n"
     ]
    }
   ],
   "source": [
    "#hard stop after 500 loops to avoid any server overload due to infinite crawling\n",
    "stop = 500 # Use only for top level of scraping \n",
    "start_time = time.time() # measure time taken to scrape data \n",
    "failure=False\n",
    "\n",
    "if CRAWL_PROFILE:\n",
    "    #1.1 Crawl through all the pages and extract profile links into a list\n",
    "    print(\"\\n1.Crawling all the pages and extracting profile links ...\")\n",
    "    print(\"\\nSeed URL : \",seed_url)\n",
    "\n",
    "    while len(queue)!=0 and stop >0:\n",
    "        try:\n",
    "            random_time=random.randint(0,1) # genrate random time wait in sec\n",
    "            #retrieve HTML content of the page\n",
    "        #     print(\"url : \",queue[0])\n",
    "            page =requests.get(queue[0])\n",
    "\n",
    "            #extract all the requried info from the page\n",
    "            if page.status_code==200:\n",
    "                extract_info_from_page(page)\n",
    "            else:\n",
    "                print(\"Failed to load page [ERROR_CODE]: \",page.status_code)\n",
    "                print(\"Page Url : \",queue[0])\n",
    "\n",
    "            #Try next link -> pop out the first page from main queue::FIFO and add into already visited url list\n",
    "            if len(queue)>0 :\n",
    "                already_visited.append(queue.pop(0))\n",
    "            stop = stop-1\n",
    "            print(\".\",end='') # print to show that scraping is in progress...\n",
    "            time.sleep(random_time)  # add delay of 1s before visiting next page\n",
    "        except:\n",
    "            print(\"Inside level 1 crawling....something went wrong..Exiting\\n\")\n",
    "            failure=True\n",
    "            break\n",
    "\n",
    "    stop_time = time.time()        \n",
    "    if failure==False:\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape Profile data :\",str(np.round(time_taken,4))+\" sec\")\n",
    "        \n",
    "        print(\"\\nSaving all the extracted profiles and articles into csv file..\")\n",
    "        #Write profile data into file\n",
    "        writeData_profile() \n",
    "        print(\"\\nSaving Completed.\")         \n",
    "    else:\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape Profile data :\",str(np.round(time_taken,4))+\" sec\")\n",
    "        \n",
    "        print(\"\\nSaving all the extracted profiles and articles into csv file..\")\n",
    "        #Write profile data into file\n",
    "        writeData_profile()         \n",
    "        print(\"Level 1: Extraction of profiles failed...Saving extracted data so far. Program exit\")\n",
    "\n",
    "if CRAWL_ARTICLE:    \n",
    "    start_time = time.time() # measure time taken to scrape data \n",
    "    #1.2 Crawl through all the profiles link now to extract articles for each users\n",
    "    profile_df =pd.read_csv(\"CoventryUni_Data_Scraped_profile.csv\",header='infer')\n",
    "    profile_queue = profile_df['Profile_Link'].values # assign the profile_queue with list of all the pofile link ::FIFO\n",
    "    head_profile_url = profile_queue[0]\n",
    "    stop = len(profile_queue)\n",
    "\n",
    "    print(\"\\nn 2. Crawling all the Profile links and extracting articles info ...\")\n",
    "    print(\"\\nFirst profile link : \",head_profile_url)\n",
    "    print(\"\\nQueue length : \",len(profile_queue))\n",
    "\n",
    "    while len(profile_queue) != 0 and stop >0:\n",
    "        try:\n",
    "            stop = stop-1\n",
    "            random_time=random.randint(0,1) # genrate random time wait in sec\n",
    "\n",
    "               # Crawl to individual user profile and extract their published articles details\n",
    "            extract_user_info(head_profile_url)\n",
    "#             print(\"Processsing done , pending \", len(profile_queue)-1)\n",
    "                #pop out the first link from the queue and assign second link as first item\n",
    "            if len(profile_queue)!=1:\n",
    "                profile_queue = profile_queue[1:]\n",
    "                #assign head of queue to head_profile_url for next crawl\n",
    "                head_profile_url = profile_queue[0]\n",
    "            else:\n",
    "                profile_queue = [] # All the links are crawled\n",
    "\n",
    "            print(len(profile_queue),end=',') # print to show that scraping is in progress...\n",
    "            time.sleep(random_time)  # add delay of 1s before visiting next page\n",
    "        except:\n",
    "            print(\"Inside level 2 crawling....something went wrong..Exiting\\n\")\n",
    "            failure = True\n",
    "            break\n",
    "\n",
    "    stop_time = time.time()\n",
    "    if failure == False:\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape article data :\",str(np.round(time_taken,4))+\" sec\")\n",
    "\n",
    "        print(\"Nothing to process. Queue Len : \",len(queue))\n",
    "        print(\"No.Of Entries extracted : \", total_entries)\n",
    "        print(\"\\nCompleted before hard stop, Iterations still left before hard stop(max 500 iterations) : \",stop)\n",
    "\n",
    "        #Write published article data into file\n",
    "        print(\"\\nSaving all the extracted articles details into csv file..\")    \n",
    "        writeData_articles()\n",
    "        print(\"\\nSaving Completed.\")  \n",
    "    else:\n",
    "        print(\"pages pending to crawl : \",len(profile_queue))\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape article data :\",str(np.round(time_taken,4))+\" sec\")        \n",
    "        writeData_articles()\n",
    "        print(\"Level 2: Extraction of articles failed.. Saving crawled data so far.. Program exit\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
